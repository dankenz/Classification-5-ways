{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "micro-cooking",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "In this notebook i am going to test 5 classification techniques in predicting fraud cases in a unbalanced dataset. The 5 ways are Logistic Regression,KNearest, Support Vector Classificator,Decision Tree Classifier and XGBoost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "constitutional-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import xgboost as xgb\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-interest",
   "metadata": {},
   "source": [
    "I had to balance a sample of the dataset to get better models.The original dataset have a majority of non-frauds. With an unbalanced dataset a \"dumb\" model will get andvantage by tagging every row as a non-fraud and get a good accuracy score. In the balanced dataset i got all the fraud cases and shuffle them between the same number of non-fraud cases, so the new dataset have 492 + 492 = 984 cases in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suspended-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing balanced data\n",
    "data = pd.read_csv('C:\\\\Users\\\\User\\\\Documents\\\\ML\\\\simula√ßoes de portfolio\\\\fraud\\\\balanced_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-disco",
   "metadata": {},
   "source": [
    "Here I prepared the data for the models. I separed the data into training data and testing data. Also the XGBoost method requires some preparation for its data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "heated-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X,y = data.drop(['Class'],axis=1) , data[['Class']]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train= np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "xg_reg = xgb.XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.3,\n",
    "                max_depth = 10, alpha = 15, n_estimators = 10, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-sewing",
   "metadata": {},
   "source": [
    "Defining the methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "decreased-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple classifiers\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"XGBoost\":xg_reg\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-campbell",
   "metadata": {},
   "source": [
    "Training and testing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "analyzed-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_score_f1=[]\n",
    "training_score_recall=[]\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score_f1.append(np.mean(cross_val_score(classifier, X_test, y_test, cv=10,scoring='f1')))\n",
    "    training_score_recall.append(np.mean(cross_val_score(classifier, X_test, y_test, cv=10,scoring='recall')))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-jesus",
   "metadata": {},
   "source": [
    "Now we need to see how the models perform. For this problem the f1 score and recall are good metrics and here is why: For fraud detection we really don't want a model that accuses non-fraud cases as fraudulent. The recall metric is calculated as (true positives)/(true positives + false negatives), wich penalises a model that perform with a lot of false negatives by giving a low score. The f1-score is calculated as (true positive)/(true positive +2(false positives + false negatives)) and also penalises models that make a lot of mistakes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "transparent-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisiticRegression</th>\n",
       "      <th>KNearest</th>\n",
       "      <th>Support Vector Classifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f1 score</th>\n",
       "      <td>0.905019</td>\n",
       "      <td>0.622881</td>\n",
       "      <td>0.651476</td>\n",
       "      <td>0.878436</td>\n",
       "      <td>0.913825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.867273</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.912727</td>\n",
       "      <td>0.912727</td>\n",
       "      <td>0.868182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LogisiticRegression  KNearest  Support Vector Classifier  \\\n",
       "f1 score             0.905019  0.622881                   0.651476   \n",
       "recall               0.867273  0.659091                   0.912727   \n",
       "\n",
       "          DecisionTreeClassifier   XGBoost  \n",
       "f1 score                0.878436  0.913825  \n",
       "recall                  0.912727  0.868182  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perff1={\"LogisiticRegression\":training_score_f1[0],\"KNearest\":training_score_f1[1],\"Support Vector Classifier\":training_score_f1[2],\"DecisionTreeClassifier\":training_score_f1[3],\"XGBoost\":training_score_f1[4]}\n",
    "perfre={\"LogisiticRegression\":training_score_recall[0],\"KNearest\":training_score_recall[1],\"Support Vector Classifier\":training_score_recall[2],\"DecisionTreeClassifier\":training_score_recall[3],\"XGBoost\":training_score_recall[4]}\n",
    "    \n",
    "    \n",
    "perf=pd.DataFrame([perff1,perfre]).rename(index={0:'f1 score',1:'recall'})\n",
    "perf.to_csv('performace.csv')\n",
    "perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-harvey",
   "metadata": {},
   "source": [
    "We can see that Logistic Regression, Decision Tree and XGBoost have a good testing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-effects",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-criminal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
